{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Due to the 100 MB size limit on GitHub, I separated the data cleaning processes into three notebooks:\n",
        "\n",
        "* sf_crime_prediction.ipynb (main notebook)\n",
        "* working_on_weather_data.ipynb\n",
        "* working_on_flight_data.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "pqhwwqKUWRFj",
        "outputId": "46df08fe-6780-446c-80f2-b033cd660905"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from ydata_profiling import ProfileReport\n",
        "import requests\n",
        "import os\n",
        "from io import StringIO\n",
        "import json\n",
        "\n",
        "import missingno as msno\n",
        "import folium\n",
        "\n",
        "\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import re\n",
        "\n",
        "from folium.plugins import MarkerCluster\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = os.getcwd()\n",
        "raw_data_path = path + \"/../data/raw/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Bu3YeqkZTgH"
      },
      "outputs": [],
      "source": [
        "# download our incident data and store in the raw data directory\n",
        "\n",
        "base_url = \"https://data.sfgov.org/resource/wg3w-h783.csv\"\n",
        "limit  = 1000\n",
        "offset = 0\n",
        "data  = []\n",
        "\n",
        "df_incident = pd.DataFrame()\n",
        "\n",
        "data_path = raw_data_path +'incident_raw.csv'\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "  while True:\n",
        "    url = f\"{base_url}?$limit={limit}&$offset={offset}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "      if not offset%100000:\n",
        "        print(f\"Fetching data from {url}\")\n",
        "      batch_df = pd.read_csv(StringIO(response.text))\n",
        "      if batch_df.empty:\n",
        "        break\n",
        "      df_incident = pd.concat([df_incident, batch_df], ignore_index=True)\n",
        "      offset += limit\n",
        "    else:\n",
        "      print('Done!')\n",
        "      break\n",
        "  df_incident.to_csv(data_path)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku6tJBipkcNP"
      },
      "outputs": [],
      "source": [
        "# create dataframe\n",
        "df_incident = pd.read_csv(data_path)\n",
        "\n",
        "print(df_incident.info())\n",
        "print(\"\")\n",
        "df_incident.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"schema-column-preview-table\">\n",
        "  <table class=\"forge-table\">\n",
        "    <thead>\n",
        "      <tr>\n",
        "        <th>Column Name</th>\n",
        "        <th>Description</th>\n",
        "      </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "      <tr>\n",
        "        <td>Incident Datetime</td>\n",
        "        <td>The date and time when the incident occurred</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Incident Date</td>\n",
        "        <td>The date the incident occurred</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Incident Time</td>\n",
        "        <td>The time the incident occurred</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Incident Year</td>\n",
        "        <td>The year the incident occurred, provided as a convenience for filtering</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Incident Day of Week</td>\n",
        "        <td>The day of the week the incident occurred</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Report Datetime</td>\n",
        "        <td>Distinct from Incident Datetime, Report Datetime is when the report was filed</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Row ID</td>\n",
        "        <td>A unique identifier for each row of data in the dataset</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Incident ID</td>\n",
        "        <td>This is the system generated identifier for incident reports</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Incident Number</td>\n",
        "        <td>The number issued on the report, used to reference cases and report documents</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>CAD Number</td>\n",
        "        <td>The Computer Aided Dispatch (CAD) is the system used by the Department of Emergency Management to dispatch officers and other public safety personnel</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Report Type Code</td>\n",
        "        <td>A system code for report types</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Report Type Description</td>\n",
        "        <td>The description of the report type, such as Initial, Initial Supplement, Vehicle Initial, etc.</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Filed Online</td>\n",
        "        <td>Indicates if the report was filed online by the public using SFPDâ€™s self-service reporting system</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Incident Code</td>\n",
        "        <td>System codes to describe a type of incident</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Incident Category</td>\n",
        "        <td>A category mapped onto the Incident Code used in statistics and reporting</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Incident Subcategory</td>\n",
        "        <td>A subcategory mapped to the Incident Code used for statistics and reporting</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Incident Description</td>\n",
        "        <td>Description of the incident that corresponds with the Incident Code</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Resolution</td>\n",
        "        <td>The resolution of the incident at the time of the report</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Intersection</td>\n",
        "        <td>The 2 or more street names that intersect closest to the original incident</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>CNN</td>\n",
        "        <td>The unique identifier of the intersection for reference back to other related basemap datasets</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Police District</td>\n",
        "        <td>The Police District where the incident occurred</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Analysis Neighborhood</td>\n",
        "        <td>The neighborhood where each incident occurs</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Supervisor District</td>\n",
        "        <td>Current Supervisor District</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Supervisor District 2012</td>\n",
        "        <td>Previous 2012-2022 Supervisor District</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Latitude</td>\n",
        "        <td>The latitude coordinate in WGS84</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Longitude</td>\n",
        "        <td>The longitude coordinate in WGS84</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>Point</td>\n",
        "        <td>Geolocation in OGC WKT format</td>\n",
        "      </tr>\n",
        "    </tbody>\n",
        "  </table>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niBkE0ffktpG"
      },
      "outputs": [],
      "source": [
        "# print the number of missing data for each column\n",
        "df_isna = pd.DataFrame(df_incident.isna().sum(), columns=['missing_data'])\n",
        "df_isna[df_isna['missing_data']!=0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del df_isna # no need to store extra data in the memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "msno.matrix(df_incident.sort_values(by='Incident Datetime'), figsize=(20, 10))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### columns: \n",
        "* `ESNCAG - Boundary File` \n",
        "* `Central Market/Tenderloin Boundary Polygon - Updated` \n",
        "* `Civic Center Harm Reduction Project Boundary` \n",
        "* `HSOC Zones as of 2018-06-05`\n",
        "* `Invest In Neighborhoods (IIN) Areas` \n",
        "### are pretty much empty columns, those columns will not help with any prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## We can start cleaning our data with dropping those columns \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident.drop(columns=['ESNCAG - Boundary File', 'Central Market/Tenderloin Boundary Polygon - Updated', 'Civic Center Harm Reduction Project Boundary', 'HSOC Zones as of 2018-06-05', 'Invest In Neighborhoods (IIN) Areas'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "msno.matrix(df_incident.sort_values(by='Incident Datetime'), figsize=(20, 10))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Check for duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a basic check for duplicates with checking Date, id and incident code \n",
        "# Later we will see there are duplicates for incident ID, and we will figure out that mistery\n",
        "duplicates = df_incident[['Incident Date', 'Incident ID',  'Incident Code']].duplicated(keep=False).sum()\n",
        "print(f\"There are {duplicates} duplicates in the dataframe.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Which columns we will drop?\n",
        "### Lets focus on the features we need for now!\n",
        "\n",
        "### keep:\n",
        "* Incident Date\n",
        "* Incident Time\n",
        "* Incident Day of Week\n",
        "* Incident Category \n",
        "* Incident Subcategory \n",
        "* Incident Description\n",
        "* Incident Code\n",
        "* Incident ID\n",
        "* Intersection \n",
        "* Latitude\n",
        "* Longtitude\n",
        "* Analysis Neighborhood\n",
        "* Report type code\n",
        "* report type description\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident  = df_incident.drop(columns=['Incident Datetime',  'Report Datetime', 'Row ID', 'Resolution', 'Report Type Code',\t'Report Type Description','CNN', 'Police District', 'Supervisor District', 'Supervisor District 2012', 'Neighborhoods','Current Supervisor Districts', 'Current Police Districts', 'Point', 'Filed Online']).sort_values(by='Incident Date')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG_MJYqqcajS"
      },
      "outputs": [],
      "source": [
        "# `CAD Number` will not be helpful to our model and we can drop that column.\n",
        "# Same for the 'Intersection'  we will continue with 'Analysis Neighborhood' column for location \n",
        "\n",
        "df_incident.drop(columns=['CAD Number', 'Intersection'], inplace=True)\n",
        "df_incident.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for any incident codes that are present in both categories: \n",
        "# 1. Incident codes with missing 'Incident Category'\n",
        "# 2. Incident codes with non-missing 'Incident Category'\n",
        "df_incident[df_incident['Incident Code'].isin(set(df_incident[df_incident['Incident Category'].isna()]['Incident Code']))]['Incident Category'].notna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## There are some Incident categories is NaN and I wanted to check if corresponding Incident ID has any corresponding Incident Category not null. \n",
        "## But looks like there is none.  Look like we can fill these missing categories according to Incident  Description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident[df_incident['Incident Code'].isin(set(df_incident[df_incident['Incident Category'].isna()]['Incident Code']))]['Incident Code'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident[df_incident['Incident Code'].isin(set(df_incident[df_incident['Incident Category'].isna()]['Incident Code']))]['Incident Code'].value_counts().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident[df_incident['Incident Code'] == 65021]['Incident Category'].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## After some exploaration I found out the incident codes above has no corresponding Incident category, and lets see what are the corresponding Incidinet Description \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "descriptions_for_missing_categories = df_incident[df_incident['Incident Code'].isin(set(df_incident[df_incident['Incident Category'].isna()]['Incident Code']))]['Incident Description'].unique()\n",
        "descriptions_for_missing_categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "years_for_missing_categories = df_incident[df_incident['Incident Code'].isin(set(df_incident[df_incident['Incident Category'].isna()]['Incident Code']))]['Incident Date']\n",
        "pd.to_datetime(years_for_missing_categories).dt.year.value_counts()\n",
        "# I could not find any correlations between year and missing data at category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for missing_description in descriptions_for_missing_categories:\n",
        "    print(f\"{missing_description}: \", end=\" \")\n",
        "    print(df_incident[(df_incident['Incident Description'] == missing_description) & (df_incident['Incident Category'].notna())].value_counts().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `Theft, Boat` from description is the only one has been in a category before.\n",
        "### Lets see what is that category:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident[df_incident['Incident Description']=='Theft, Boat']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### We can change Incident code to 'Larceny Theft' for corresponding to  where 'Incident Description\" is 'Theft, Boat. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident.loc[df_incident['Incident Description']=='Theft, Boat',['Incident Category', 'Incident Subcategory']] = 'Larceny Theft'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## For the remaining missing data, I will use AI (OpenAI) to classify the corresponding Incident Category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "\n",
        "client = OpenAI(api_key=str(os.getenv('OPENAI_API_KEY')))\n",
        "\n",
        "\n",
        "model = 'gpt-4o'\n",
        "\n",
        "instructions = f\"\"\"You will receive a list of crime descriptions, and you will find the best possible corresponding categories from \n",
        "{set(df_incident[df_incident['Incident Category'].notnull()]['Incident Category'])}.\n",
        "Your response should be in the format of a Python dictionary with description:category pairs. with not using new line, just one line\"\"\"\n",
        "\n",
        "def get_comtpetion(prompt, model=model, instruction=instructions):\n",
        "    message=[{'role':'system', 'content':instruction}, \n",
        "             {'role':'user', 'content':prompt}]\n",
        "    response=client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=message,\n",
        "        temperature = 1,\n",
        "        \n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "missing_descriptions = set(df_incident[df_incident['Incident Code'].isin(set(df_incident[df_incident['Incident Category'].isna()]['Incident Code']))]['Incident Description'])\n",
        "\n",
        "\n",
        "\n",
        "text_dict = get_comtpetion(f\"Here are the descriptions for you: {list(missing_descriptions)}\")\n",
        "\n",
        "match = re.search(r\"\\{(.*?)\\}\", text_dict.strip(), re.DOTALL) \n",
        "if match:\n",
        "    dict_text = '{'+match.group(1)+'}'\n",
        "else:\n",
        "    print('No match')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "description_to_category =eval(dict_text)\n",
        "for key, value in description_to_category.items():\n",
        "    df_incident.loc[df_incident['Incident Description'] == key , ['Incident Category', 'Incident Subcategory']] = value\n",
        "\n",
        "df_incident.isna().sum()    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's Fill Empty Cells in Latitude, Longitude, and 'Analysis Neighborhood' Columns\n",
        "\n",
        "### My Plan:\n",
        "\n",
        "1. **Filling 'Analysis Neighborhood':**\n",
        "   - Start by filling the 'Analysis Neighborhood' column with the mode (most frequent) neighborhood for the corresponding incident category.\n",
        "\n",
        "2. **Filling Latitude and Longitude:**\n",
        "   - Fill the Latitude and Longitude columns with the mean values of their respective groups, grouped by the first neighborhood and incident category.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "mode_neighborhoods = df_incident.groupby('Incident Category')['Analysis Neighborhood'].agg(lambda x: x.mode()[0])\n",
        "\n",
        "cat_neig = mode_neighborhoods.to_dict()\n",
        "\n",
        "\n",
        "for index, row in df_incident.iterrows():\n",
        "    if pd.isna(row['Analysis Neighborhood']):\n",
        "        df_incident.loc[index, 'Analysis Neighborhood'] = cat_neig[row['Incident Category']]\n",
        "        \n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident['Latitude'] = df_incident.groupby('Analysis Neighborhood')['Latitude'].transform(lambda x: x.fillna(x.mean()))\n",
        "df_incident['Longitude'] = df_incident.groupby('Analysis Neighborhood')['Longitude'].transform(lambda x: x.fillna(x.mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# finally fix the data type for 'Incident Date' \n",
        "df_incident['Incident Date'] = pd.to_datetime(df_incident['Incident Date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "category_counts = df_incident['Incident Category'].value_counts()\n",
        "sorted_categories = category_counts.index.to_list()\n",
        "sns.countplot(data=df_incident,\n",
        "             x='Incident Category', order=sorted_categories)\n",
        "plt.title(\"Incident Categories Density in Order from Highest to Lowest\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## There are too many different Incident categories which are not being so helpful. I will focus on that later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lets check consistincy\n",
        "df_incident[df_incident['Incident Description'] == 'Battery']['Incident Category'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lets merge our dataframes (df_incident & df_weather)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_weather = pd.read_csv(path + \"/../data/processed/sf_weather_processed.csv\")\n",
        "df_weather['DATE']= pd.to_datetime(df_weather['DATE'], format=\"%Y-%m-%d\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we will merge our data on date coumns\n",
        "\n",
        "df_incident = pd.merge(df_incident, df_weather, left_on='Incident Date', right_on='DATE', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del df_weather\n",
        "df_incident = df_incident.drop(columns=['DATE'])\n",
        "df_incident.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lets rename our columns for consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident.rename(columns={\n",
        "    'Incident Date': 'incident_date',\n",
        "    'Incident Time': 'incident_time',\n",
        "    'Incident Year': 'incident_year',\n",
        "    'Incident Day of Week': 'incident_day',\n",
        "    'Incident ID': 'incident_id',\n",
        "    'Incident Number': 'incident_no',\n",
        "    'Incident Code': 'incident_code',\n",
        "    'Incident Category': 'category',\n",
        "    'Incident Subcategory': 'subcategory',\n",
        "    'Incident Description': 'description',\n",
        "    'Analysis Neighborhood': 'neighborhood',\n",
        "    'Latitude': 'latitude',\n",
        "    'Longitude': 'longitude',\n",
        "    'PRCP': 'precipitation',\n",
        "    'TMIN': 'min_temperature',\n",
        "    'TMAX': 'max_temperature'\n",
        "}, inplace=True)\n",
        "\n",
        "df_incident.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Looks good! Now time to get our flight data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_flight = pd.read_csv(path+\"/../data/processed/flight_processed.csv\")\n",
        "df_flight['activity_period'] = pd.to_datetime(df_flight['activity_period'], format=\"%Y-%m-%d\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Merge flight data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident = df_incident.merge(df_flight, left_on='incident_date', right_on='activity_period', how='left')\n",
        "del df_flight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident[['activity_period','deplaned_passenger','enplaned_passenger']] = df_incident[['activity_period','deplaned_passenger','enplaned_passenger']].ffill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we have missing data on weather related columns Lets see what are the corresponding dates to those\n",
        "\n",
        "df_incident[df_incident['precipitation'].isna()]['incident_date'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### We only have data missing for 4 days related to weather. It will be acceptable to fill these cells with data from earlier days, as the weather conditions are likely to be quite similar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident.ffill(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident.drop(columns='activity_period', inplace=True)\n",
        "df_incident.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perfect! We merged all there tables and we handled all the missing datas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Now we need to work on incidents. Many incidents consist of multiple sub-incidents. We need to filter these incidents into one main incident category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident[df_incident['incident_id'].duplicated(keep=False)][['incident_id', 'category', 'subcategory']].sort_values('incident_id').head(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sorted(list(df_incident['category'].unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### There is the same category, but with different British and American spellings: 'Weapons Offence' versus 'Weapons Offense.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident.loc[df_incident['category']=='Weapons Offense', 'category'] = 'Weapons Offence'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### There are some categories I want to see their description to understand better. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident[(df_incident['category'].isin(['Suspicious','Suspicious Occ'])) & \\\n",
        "    ~(df_incident['description'].isin(['Suspicious Occurrence','Suspicious Vehicle' ,'Suspicious Person','Suspicious Act Towards Female']) )]\\\n",
        "        [['subcategory', 'description']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident[df_incident['category']=='Other Offenses']['description']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Firsty I will drop the rows has crimes has no require immidiate police attention or no police attention at all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident = df_incident[(~df_incident['category'].isin(['Case Closure', 'Non-Criminal' , 'Offences Against The Family And Children','Courtesy Report', 'Fire Report','Recovered Vehicle'\n",
        "                                          ,'Warrant', 'Liquor Laws', 'Other Miscellaneous', 'Gambling', 'Other', 'Missing Person', \\\n",
        "                                              'Civil Sidewalks', 'Traffic Collision', 'Suicide','Suspicious','Suspicious Occ', 'Other Offenses']))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sorted(list(df_incident['category'].unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's see crime distribution on map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_on_map_with_category(cat_type):\n",
        "    sf_map= folium.Map(location=[37.7749, -122.4194], zoom_start=12)\n",
        "\n",
        "    marker_cluster = MarkerCluster().add_to(sf_map)\n",
        "    \n",
        "    for i, row in df_incident.loc[df_incident['category']==cat_type].iterrows():\n",
        "        folium.Marker(\n",
        "            location=[row['latitude'], row['longitude']],\n",
        "            popup=row['incident_date'].strftime('%B %d,%Y')\n",
        "        ).add_to(marker_cluster)\n",
        "    display(sf_map)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(df_incident[(df_incident['category']=='Malicious Mischief') & (df_incident['subcategory']=='Other')][['subcategory', 'description']].head())\n",
        "df_incident[df_incident['category']=='Malicious Mischief'][['subcategory']].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Looks like we can Change `Malicious Mischief` to `Vandalism` after droping wher subcategory is Other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident = df_incident[~((df_incident['category']=='Malicious Mischief') & (df_incident['subcategory']=='Other'))]\n",
        "df_incident.loc[df_incident['category']=='Malicious Mischief', 'category'] = 'Vandalism'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_on_map_with_subcategory(cat_type):\n",
        "    sf_map= folium.Map(location=[37.7749, -122.4194], zoom_start=12)\n",
        "\n",
        "    marker_cluster = MarkerCluster().add_to(sf_map)\n",
        "    \n",
        "    for i, row in df_incident.loc[df_incident['subcategory']==cat_type].iterrows():\n",
        "        folium.Marker(\n",
        "            location=[row['latitude'], row['longitude']],\n",
        "            popup=row['incident_date'].strftime('%B %d,%Y')\n",
        "        ).add_to(marker_cluster)\n",
        "    return(sf_map)\n",
        "    \n",
        "sf_map =find_on_map_with_subcategory('Larceny - From Vehicle')\n",
        "sf_map.save('larceny_from_vehicle_on_map.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(sf_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wow! That is more than what I expected, I hope our prediction model will work!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sorted_categories = df_incident['category'].value_counts().index.to_list()\n",
        "sorted_categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# all the day name matches with the actual date\n",
        "(~(df_incident['incident_day'] == df_incident['incident_date'].dt.day_name())).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident['category'] = pd.Categorical(df_incident['category'], categories=sorted_categories, ordered=True)\n",
        "df_incident['subcategory'] = pd.Categorical(df_incident['subcategory'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident = df_incident.sort_values(by=['incident_id', 'category'])\n",
        "df_incident.drop_duplicates(subset='incident_id', keep='first', inplace=True)\n",
        "df_incident.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident.sort_values(by='incident_date', inplace=True)\n",
        "df_incident.drop(columns=['incident_id', 'incident_code'], inplace=True)\n",
        "print(df_incident.shape)\n",
        "df_incident.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "profile = ProfileReport(df_incident, title=\"Profiling Report\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "profile.to_file('sf_crime_clean_data_report.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns = df_incident.columns.to_list()\n",
        "data_types = [df_incident[column].dtypes.name for column in columns]\n",
        "data_type_dict = dict(zip(columns, data_types))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident.reset_index(drop=True, inplace=True)\n",
        "df_incident.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_incident.to_csv(path+'/../data/interim/sf_crime_cleaned.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(path+'/../data/interim/sf_incident_dtypes.json', 'w') as f:\n",
        "    json.dump(data_type_dict, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
